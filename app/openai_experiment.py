"""
ThÃ­ nghiá»‡m Reducing Hallucinations vá»›i OpenAI GPT
Thay tháº¿ cho Gemini náº¿u khÃ´ng cÃ³ API access
"""
import os, json, re
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm
from difflib import SequenceMatcher
from openai import OpenAI

# --- 1. Cáº¤U HÃŒNH ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

if not OPENAI_API_KEY:
    print("KhÃ´ng tÃ¬m tháº¥y OPENAI_API_KEY trong file .env")
    exit()

client = OpenAI(api_key=OPENAI_API_KEY)

# --- 2. Äá»ŠNH NGHÄ¨A PROMPT ---

def get_baseline_prompt(question):
    """Tráº£ vá» prompt Ä‘Æ¡n giáº£n"""
    return f"HÃ£y tráº£ lá»i cÃ¢u há»i sau má»™t cÃ¡ch ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c: {question}"

def get_critique_prompt(question):
    """Tráº£ vá» prompt tá»± pháº£n biá»‡n 3 bÆ°á»›c"""
    return f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI cáº©n trá»ng, luÃ´n kiá»ƒm tra láº¡i thÃ´ng tin.
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i sau báº±ng quy trÃ¬nh 3 bÆ°á»›c.

CÃ¢u há»i: {question}

---
[Báº®T Äáº¦U QUY TRÃŒNH]

**BÆ°á»›c 1: CÃ¢u tráº£ lá»i ban Ä‘áº§u:**
[HÃ£y táº¡o cÃ¢u tráº£ lá»i ban Ä‘áº§u cá»§a báº¡n á»Ÿ Ä‘Ã¢y]

**BÆ°á»›c 2: Tá»± pháº£n biá»‡n:**
[HÃ£y xem xÃ©t láº¡i cÃ¢u tráº£ lá»i á»Ÿ BÆ°á»›c 1. NÃ³ cÃ³ chÃ­nh xÃ¡c khÃ´ng? CÃ³ "hallucinate" Ä‘iá»ƒm nÃ o khÃ´ng? CÃ³ thá»ƒ cáº£i thiá»‡n á»Ÿ Ä‘Ã¢u?]

**BÆ°á»›c 3: CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng (Ä‘Ã£ xÃ¡c minh):**
[Dá»±a trÃªn pháº£n biá»‡n á»Ÿ BÆ°á»›c 2, hÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng, chÃ­nh xÃ¡c nháº¥t.]
"""

def extract_final_answer(critique_text):
    """TrÃ­ch xuáº¥t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng tá»« BÆ°á»›c 3"""
    patterns = [
        r'\*\*BÆ°á»›c 3[:\s]+.*?\*\*[:\s]*(.*?)(?=\n\n|\Z)',
        r'CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng[:\s]*\*\*[:\s]*(.*?)(?=\n\n|\Z)',
        r'BÆ°á»›c 3[:\s]+(.*?)(?=\n\n|\Z)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, critique_text, re.DOTALL | re.IGNORECASE)
        if match:
            answer = match.group(1).strip()
            answer = answer.strip('*[](){}')
            return answer
    
    return critique_text.strip()

def calculate_similarity(text1, text2):
    """TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 string (0-1)"""
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def evaluate_answer(predicted, ground_truth, threshold=0.6):
    """ÄÃ¡nh giÃ¡ cÃ¢u tráº£ lá»i"""
    if predicted.lower().strip() == ground_truth.lower().strip():
        return True, 1.0
    
    sim = calculate_similarity(predicted, ground_truth)
    is_correct = sim >= threshold
    
    return is_correct, sim

# --- 3. Táº¢I DATASET ---

DATA_DIR = Path("data")
if not (DATA_DIR / "questions.json").exists():
    print("KhÃ´ng tÃ¬m tháº¥y file questions.json hoáº·c answers.json trong thÆ° má»¥c data/")
    exit()

all_questions = json.loads((DATA_DIR / "questions.json").read_text(encoding="utf-8"))
all_answers_ground_truth = json.loads((DATA_DIR / "answers.json").read_text(encoding="utf-8"))

NUM_SAMPLES = 10  # Báº¯t Ä‘áº§u vá»›i 10 máº«u

benchmark_data = []
for i in range(min(NUM_SAMPLES, len(all_questions))):
    benchmark_data.append({
        "id": i,
        "question": all_questions[i],
        "ground_truth": all_answers_ground_truth[i]
    })

print(f"ÄÃ£ táº£i {len(benchmark_data)} cÃ¢u há»i tá»« ViQuAD Ä‘á»ƒ lÃ m benchmark.")
print(f"Báº¯t Ä‘áº§u cháº¡y thÃ­ nghiá»‡m so sÃ¡nh Baseline vs Self-Critique...\n")

# --- 4. CHáº Y THÃ NGHIá»†M ---

results = []

for item in tqdm(benchmark_data, desc="Äang cháº¡y thÃ­ nghiá»‡m"):
    q = item["question"]
    gt = item["ground_truth"]
    
    # 1. Cháº¡y Baseline
    try:
        response_bl = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": get_baseline_prompt(q)}],
            temperature=0
        )
        answer_bl = response_bl.choices[0].message.content.strip()
    except Exception as e:
        print(f"\nLá»—i khi cháº¡y Baseline cÃ¢u {item['id']}: {e}")
        answer_bl = f"[Lá»–I: {e}]"

    # 2. Cháº¡y Self-Critique
    try:
        response_sc = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": get_critique_prompt(q)}],
            temperature=0
        )
        answer_sc_full = response_sc.choices[0].message.content.strip()
        answer_sc_final = extract_final_answer(answer_sc_full)
    except Exception as e:
        print(f"\nLá»—i khi cháº¡y Self-Critique cÃ¢u {item['id']}: {e}")
        answer_sc_full = f"[Lá»–I: {e}]"
        answer_sc_final = f"[Lá»–I: {e}]"

    # 3. ÄÃ¡nh giÃ¡
    bl_correct, bl_sim = evaluate_answer(answer_bl, gt)
    sc_correct, sc_sim = evaluate_answer(answer_sc_final, gt)

    # 4. LÆ°u káº¿t quáº£
    results.append({
        "id": item["id"],
        "question": q,
        "ground_truth": gt,
        "baseline_answer": answer_bl,
        "baseline_correct": bl_correct,
        "baseline_similarity": bl_sim,
        "critique_answer_full": answer_sc_full,
        "critique_answer_final": answer_sc_final,
        "critique_correct": sc_correct,
        "critique_similarity": sc_sim
    })

# --- 5. PHÃ‚N TÃCH Káº¾T QUáº¢ ---
df_results = pd.DataFrame(results)

baseline_accuracy = df_results['baseline_correct'].sum() / len(df_results) * 100
critique_accuracy = df_results['critique_correct'].sum() / len(df_results) * 100

baseline_avg_similarity = df_results['baseline_similarity'].mean()
critique_avg_similarity = df_results['critique_similarity'].mean()

accuracy_improvement = critique_accuracy - baseline_accuracy
relative_improvement = (accuracy_improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else 0

sc_better = (df_results['critique_similarity'] > df_results['baseline_similarity']).sum()
bl_better = (df_results['baseline_similarity'] > df_results['critique_similarity']).sum()
equal = (df_results['baseline_similarity'] == df_results['critique_similarity']).sum()

# --- 6. LÆ¯U Káº¾T QUáº¢ ---
output_file = "experiment_results_openai.csv"
df_results.to_csv(output_file, index=False, encoding="utf-8-sig")

# --- 7. IN BÃO CÃO ---
print("\n" + "="*70)
print("ğŸ“Š Káº¾T QUáº¢ THÃ NGHIá»†M: REDUCING HALLUCINATIONS Vá»šI SELF-CRITIQUE")
print("="*70)
print(f"\nğŸ“Œ ThÃ´ng tin thÃ­ nghiá»‡m:")
print(f"   â€¢ Sá»‘ cÃ¢u há»i test: {len(df_results)}")
print(f"   â€¢ Dataset: ViQuAD")
print(f"   â€¢ Model: GPT-4o-mini (OpenAI)")
print(f"   â€¢ Similarity threshold: 0.6 (60%)")

print(f"\nğŸ“ˆ Káº¾T QUáº¢ CHÃNH:")
print(f"\n   1ï¸âƒ£  BASELINE (Direct Prompt):")
print(f"      â€¢ Accuracy: {baseline_accuracy:.2f}%")
print(f"      â€¢ Average Similarity: {baseline_avg_similarity:.4f}")
print(f"      â€¢ Sá»‘ cÃ¢u tráº£ lá»i Ä‘Ãºng: {df_results['baseline_correct'].sum()}/{len(df_results)}")

print(f"\n   2ï¸âƒ£  SELF-CRITIQUE (3-Step Prompt):")
print(f"      â€¢ Accuracy: {critique_accuracy:.2f}%")
print(f"      â€¢ Average Similarity: {critique_avg_similarity:.4f}")
print(f"      â€¢ Sá»‘ cÃ¢u tráº£ lá»i Ä‘Ãºng: {df_results['critique_correct'].sum()}/{len(df_results)}")

print(f"\n   âœ¨ IMPROVEMENT:")
print(f"      â€¢ Accuracy Improvement: {accuracy_improvement:+.2f}% (absolute)")
print(f"      â€¢ Relative Improvement: {relative_improvement:+.2f}%")
print(f"      â€¢ Similarity Improvement: {critique_avg_similarity - baseline_avg_similarity:+.4f}")

print(f"\n   ğŸ“Š So sÃ¡nh tá»«ng cÃ¢u:")
print(f"      â€¢ Self-Critique tá»‘t hÆ¡n: {sc_better} cÃ¢u ({sc_better/len(df_results)*100:.1f}%)")
print(f"      â€¢ Baseline tá»‘t hÆ¡n: {bl_better} cÃ¢u ({bl_better/len(df_results)*100:.1f}%)")
print(f"      â€¢ Báº±ng nhau: {equal} cÃ¢u ({equal/len(df_results)*100:.1f}%)")

print(f"\nğŸ’¾ Chi tiáº¿t Ä‘áº§y Ä‘á»§ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_file}")
print("="*70)

print("\nâœ… HOÃ€N Táº¤T THÃ NGHIá»†M!")

