import google.generativeai as genai
import os, json, re
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm
from difflib import SequenceMatcher # ƒê·ªÉ ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng
import glob # ƒê·ªÉ t√¨m file benchmark

# --- 1. C·∫§U H√åNH ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()

if not GEMINI_API_KEY:
    print("Kh√¥ng t√¨m th·∫•y GEMINI_API_KEY trong file .env")
    exit()

try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    print(f"L·ªói c·∫•u h√¨nh Gemini: {e}")
    exit()

# C·∫•u h√¨nh model (d√πng cho c·∫£ 2 prompt)
config = genai.types.GenerationConfig(temperature=0.0)

# Safety settings - (Gi·ªØ nguy√™n format ƒë√∫ng c·ªßa b·∫°n)
safety = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
]

print("ƒêang kh·ªüi t·∫°o model 'gemini-1.5-pro-latest'...")
model = genai.GenerativeModel(
    'gemini-1.5-pro-latest', # THAY ƒê·ªîI: D√πng 1.5 Pro cho m·∫°nh m·∫Ω h∆°n
    generation_config=config,
    safety_settings=safety
)
print("Model ƒë√£ s·∫µn s√†ng.")

# --- 2. C√ÅC H√ÄM HELPERS (Gi·ªØ nguy√™n logic c·ªßa b·∫°n) ---

def get_baseline_prompt(question):
    """Tr·∫£ v·ªÅ prompt ƒë∆°n gi·∫£n"""
    return f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c: {question}"

def get_critique_prompt(question):
    """Tr·∫£ v·ªÅ prompt t·ª± ph·∫£n bi·ªán 3 b∆∞·ªõc"""
    return f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI c·∫©n tr·ªçng, lu√¥n ki·ªÉm tra l·∫°i th√¥ng tin.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng quy tr√¨nh 3 b∆∞·ªõc.

C√¢u h·ªèi: {question}

---
[B·∫ÆT ƒê·∫¶U QUY TR√åNH]

**B∆∞·ªõc 1: C√¢u tr·∫£ l·ªùi ban ƒë·∫ßu:**
[H√£y t·∫°o c√¢u tr·∫£ l·ªùi ban ƒë·∫ßu c·ªßa b·∫°n ·ªü ƒë√¢y]

**B∆∞·ªõc 2: T·ª± ph·∫£n bi·ªán:**
[H√£y xem x√©t l·∫°i c√¢u tr·∫£ l·ªùi ·ªü B∆∞·ªõc 1. N√≥ c√≥ ch√≠nh x√°c kh√¥ng? C√≥ "hallucinate" ƒëi·ªÉm n√†o kh√¥ng? C√≥ th·ªÉ c·∫£i thi·ªán ·ªü ƒë√¢u?]

**B∆∞·ªõc 3: C√¢u tr·∫£ l·ªùi cu·ªëi c√πng (ƒë√£ x√°c minh):**
[D·ª±a tr√™n ph·∫£n bi·ªán ·ªü B∆∞·ªõc 2, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi cu·ªëi c√πng, ch√≠nh x√°c nh·∫•t.]
"""

def extract_final_answer(critique_text):
    """
    Tr√≠ch xu·∫•t c√¢u tr·∫£ l·ªùi cu·ªëi c√πng t·ª´ B∆∞·ªõc 3 trong self-critique output
    """
    # Regex t√¨m "B∆∞·ªõc 3" (ho·∫∑c bi·∫øn th·ªÉ) v√† l·∫•y n·ªôi dung sau n√≥
    patterns = [
        # Match 'B∆∞·ªõc 3: [n·ªôi dung]'
        r'\*\*B∆∞·ªõc 3[:\s]+.*?\*\*[:\s]*(.*?)(?=\n\n|\n\*\*\s*B∆∞·ªõc|\Z)', 
        # Match 'C√¢u tr·∫£ l·ªùi cu·ªëi c√πng: [n·ªôi dung]'
        r'C√¢u tr·∫£ l·ªùi cu·ªëi c√πng[:\s]*\*\*[:\s]*(.*?)(?=\n\n|\n\*\*\s*B∆∞·ªõc|\Z)',
        # Match 'B∆∞·ªõc 3' kh√¥ng c√≥ **
        r'B∆∞·ªõc 3[:\s]+(.*?)(?=\n\n|\nB∆∞·ªõc|\Z)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, critique_text, re.DOTALL | re.IGNORECASE)
        if match:
            answer = match.group(1).strip()
            # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·∫ßu/cu·ªëi
            answer = answer.strip('*[](){}_- ')
            if answer: # ƒê·∫£m b·∫£o kh√¥ng ph·∫£i string r·ªóng
                return answer
    
    # Fallback: N·∫øu kh√¥ng t√¨m th·∫•y, c·ªë g·∫Øng l·∫•y d√≤ng cu·ªëi c√πng
    last_line = critique_text.split('\n')[-1].strip().strip('*[](){}_- ')
    if last_line:
        return last_line
        
    return critique_text.strip() # Fallback cu·ªëi c√πng

def calculate_similarity(text1, text2):
    """
    T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 string (0-1)
    """
    return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()

def evaluate_answer(predicted, ground_truth, threshold=0.6):
    """
    ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi d·ª± ƒëo√°n so v·ªõi ground truth
    Returns: (is_correct, similarity_score)
    """
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p predicted ho·∫∑c ground_truth l√† None
    predicted_str = str(predicted).lower().strip()
    ground_truth_str = str(ground_truth).lower().strip()

    if not predicted_str or not ground_truth_str:
        return False, 0.0

    # Exact match (case-insensitive)
    if predicted_str == ground_truth_str:
        return True, 1.0
    
    # Partial match v·ªõi similarity
    sim = calculate_similarity(predicted_str, ground_truth_str)
    is_correct = sim >= threshold
    
    return is_correct, sim

# --- 3. H√ÄM CH·∫†Y TH√ç NGHI·ªÜM CH√çNH ---

def run_and_evaluate_dataset(benchmark_path, similarity_threshold=0.6):
    """
    H√†m ch√≠nh: ƒê·ªçc 1 file benchmark, ch·∫°y, ƒë√°nh gi√°, v√† in b√°o c√°o.
    """
    
    dataset_name = benchmark_path.stem.replace("benchmark_", "")
    print("\n" + "="*70)
    print(f"üìä B·∫ÆT ƒê·∫¶U TH√ç NGHI·ªÜM V·ªöI DATASET: {dataset_name.upper()}")
    print("="*70)

    # ƒê·ªçc benchmark CSV
    try:
        benchmark_df = pd.read_csv(benchmark_path)
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file {benchmark_path}: {e}")
        return

    results = [] # N∆°i l∆∞u tr·ªØ t·∫•t c·∫£ k·∫øt qu·∫£
    
    # Ch·∫°y qua t·ª´ng h√†ng trong file benchmark
    for _, item in tqdm(benchmark_df.iterrows(), total=len(benchmark_df), desc=f"   -> ƒêang ch·∫°y {dataset_name}"):
        q = item["question"]
        gt = item["ground_truth"]
        
        # 1. Ch·∫°y Baseline
        try:
            prompt_bl = get_baseline_prompt(q)
            response_bl = model.generate_content(prompt_bl)
            answer_bl = response_bl.text.strip()
        except Exception as e:
            answer_bl = f"[L·ªñI: {e}]"

        # 2. Ch·∫°y Self-Critique
        try:
            prompt_sc = get_critique_prompt(q)
            response_sc = model.generate_content(prompt_sc)
            answer_sc_full = response_sc.text.strip() # To√†n b·ªô output
            answer_sc_final = extract_final_answer(answer_sc_full) # Ch·ªâ l·∫•y c√¢u tr·∫£ l·ªùi cu·ªëi
        except Exception as e:
            answer_sc_full = f"[L·ªñI: {e}]"
            answer_sc_final = f"[L·ªñI: {e}]"

        # 3. ƒê√°nh gi√° c·∫£ 2 ph∆∞∆°ng ph√°p
        bl_correct, bl_sim = evaluate_answer(answer_bl, gt, similarity_threshold)
        sc_correct, sc_sim = evaluate_answer(answer_sc_final, gt, similarity_threshold)

        # 4. L∆∞u k·∫øt qu·∫£
        results.append({
            "question": q,
            "ground_truth": gt,
            "baseline_answer": answer_bl,
            "baseline_correct": bl_correct,
            "baseline_similarity": bl_sim,
            "critique_answer_full": answer_sc_full,
            "critique_answer_final": answer_sc_final,
            "critique_correct": sc_correct,
            "critique_similarity": sc_sim
        })

    # --- 5. PH√ÇN T√çCH K·∫æT QU·∫¢ (CHO DATASET N√ÄY) ---
    df_results = pd.DataFrame(results)

    # T√≠nh to√°n c√°c metrics
    baseline_accuracy = df_results['baseline_correct'].sum() / len(df_results) * 100
    critique_accuracy = df_results['critique_correct'].sum() / len(df_results) * 100

    baseline_avg_similarity = df_results['baseline_similarity'].mean()
    critique_avg_similarity = df_results['critique_similarity'].mean()

    # C·∫£i thi·ªán
    accuracy_improvement = critique_accuracy - baseline_accuracy
    relative_improvement = (accuracy_improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else (100.0 if accuracy_improvement > 0 else 0.0)

    # ƒê·∫øm s·ªë c√¢u
    sc_better = (df_results['critique_similarity'] > df_results['baseline_similarity']).sum()
    bl_better = (df_results['baseline_similarity'] > df_results['critique_similarity']).sum()
    equal = (df_results['baseline_similarity'] == df_results['critique_similarity']).sum()

    # --- 6. L∆ØU K·∫æT QU·∫¢ RA FILE (CHO DATASET N√ÄY) ---
    output_csv_file = Path("results") / f"results_{dataset_name}.csv"
    output_txt_file = Path("results") / f"summary_{dataset_name}.txt"
    
    df_results.to_csv(output_csv_file, index=False, encoding="utf-8-sig")

    # --- 7. T·∫†O V√Ä IN B√ÅO C√ÅO (CHO DATASET N√ÄY) ---
    summary_report = f"""
=== B√ÅO C√ÅO NGHI√äN C·ª®U: REDUCING HALLUCINATIONS ===

** 1. RESEARCH QUESTION **
Li·ªáu k·ªπ thu·∫≠t Self-Critique prompting c√≥ gi·∫£m hallucination v√† c·∫£i thi·ªán 
factual accuracy so v·ªõi direct prompting kh√¥ng?

** 2. METHODOLOGY **
- Dataset: {dataset_name.upper()} ({len(df_results)} c√¢u h·ªèi)
- Model: Gemini 1.5 Pro
- Baseline: Direct prompt ƒë∆°n gi·∫£n
- Treatment: Self-Critique 3-step prompt (Initial Answer ‚Üí Critique ‚Üí Final Answer)
- Evaluation: Similarity score v·ªõi ground truth (threshold = {similarity_threshold})

** 3. RESULTS **

Baseline Accuracy:       {baseline_accuracy:.2f}%
Self-Critique Accuracy:  {critique_accuracy:.2f}%
Improvement:             {accuracy_improvement:+.2f}% (absolute)
Relative Improvement:    {relative_improvement:+.2f}%

Average Similarity:
  - Baseline:        {baseline_avg_similarity:.4f}
  - Self-Critique: {critique_avg_similarity:.4f}
  - Difference:      {critique_avg_similarity - baseline_avg_similarity:+.4f}

** 4. CONCLUSION **
"""

    if accuracy_improvement > 0:
        summary_report += f"""
Self-Critique prompting ƒê√É TH√ÄNH C√îNG trong vi·ªác gi·∫£m hallucination, 
c·∫£i thi·ªán accuracy {accuracy_improvement:.2f}% so v·ªõi baseline.
K·ªπ thu·∫≠t n√†y cho th·∫•y ti·ªÅm nƒÉng trong vi·ªác tƒÉng ƒë·ªô tin c·∫≠y c·ªßa LLM responses.
"""
    elif accuracy_improvement == 0:
        summary_report += f"""
Self-Critique prompting KH√îNG CHO TH·∫§Y S·ª∞ KH√ÅC BI·ªÜT so v·ªõi baseline.
C·∫ßn xem x√©t k·ªπ h∆°n c√°c tr∆∞·ªùng h·ª£p c·ª• th·ªÉ.
"""
    else:
        summary_report += f"""
Self-Critique prompting cho k·∫øt qu·∫£ K√âM H∆†N baseline ({accuracy_improvement:.2f}%) trong th√≠ nghi·ªám n√†y.
C√≥ th·ªÉ prompt design ch∆∞a t·ªëi ∆∞u ho·∫∑c model g·∫∑p kh√≥ khƒÉn trong vi·ªác t·ª± s·ª≠a l·ªói.
"""

    summary_report += f"""

** 5. DETAILED BREAKDOWN **
Self-Critique performs better: {sc_better} cases ({sc_better/len(df_results)*100:.1f}%)
Baseline performs better:      {bl_better} cases ({bl_better/len(df_results)*100:.1f}%)
Equal performance:           {equal} cases ({equal/len(df_results)*100:.1f}%)

===================================================
"""

    # L∆∞u summary report
    with open(output_txt_file, "w", encoding="utf-8") as f:
        f.write(summary_report)

    # In b√°o c√°o ra console
    print(summary_report)
    print(f"üíæ Chi ti·∫øt ƒë·∫ßy ƒë·ªß ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_csv_file}")
    print(f"üìÑ B√°o c√°o t√≥m t·∫Øt ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_txt_file}")


# --- 4. H√ÄM MAIN ƒê·ªÇ CH·∫†Y T·∫§T C·∫¢ DATASET ---
if __name__ == "__main__":
    
    # T·∫°o th∆∞ m·ª•c data/ v√† results/ n·∫øu ch∆∞a c√≥
    Path("data").mkdir(exist_ok=True)
    Path("results").mkdir(exist_ok=True)

    # T√¨m t·∫•t c·∫£ c√°c file benchmark ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã
    benchmark_files = list(Path("data").glob("benchmark_*.csv"))
    
    if not benchmark_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file benchmark n√†o trong th∆∞ m·ª•c 'data/'.")
        print("‚û°Ô∏è  Vui l√≤ng ch·∫°y 'python app/prepare_data.py' tr∆∞·ªõc ti√™n.")
        exit()
        
    print(f"T√¨m th·∫•y {len(benchmark_files)} b·ªô dataset benchmark ƒë·ªÉ ch·∫°y:")
    for f in benchmark_files:
        print(f"  - {f.name}")

    # L·∫∑p qua t·ª´ng file benchmark v√† ch·∫°y th√≠ nghi·ªám
    for benchmark_path in benchmark_files:
        run_and_evaluate_dataset(benchmark_path, similarity_threshold=0.6)

    print("\nüéâüéâüéâ T·∫•t c·∫£ 5 th√≠ nghi·ªám ƒë√£ ho√†n t·∫•t! üéâüéâüéâ")
    print("Ki·ªÉm tra th∆∞ m·ª•c 'results/' ƒë·ªÉ xem 5 file CSV v√† 5 file TXT b√°o c√°o.")
