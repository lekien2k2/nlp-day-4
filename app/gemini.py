import google.generativeai as genai
import os, json, re
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm
from difflib import SequenceMatcher # Äá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng
import glob # Äá»ƒ tÃ¬m file benchmark

# --- 1. Cáº¤U HÃŒNH ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()

if not GEMINI_API_KEY:
    print("KhÃ´ng tÃ¬m tháº¥y GEMINI_API_KEY trong file .env")
    exit()

try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    print(f"Lá»—i cáº¥u hÃ¬nh Gemini: {e}")
    exit()

# Cáº¥u hÃ¬nh model (dÃ¹ng cho cáº£ 2 prompt)
config = genai.types.GenerationConfig(temperature=0.0)

# Safety settings - (Giá»¯ nguyÃªn format Ä‘Ãºng cá»§a báº¡n)
safety = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
]

print("Äang khá»Ÿi táº¡o model 'gemini-1.5-pro-latest'...")
model = genai.GenerativeModel(
    'gemini-1.5-pro-latest', # THAY Äá»”I: DÃ¹ng 1.5 Pro cho máº¡nh máº½ hÆ¡n
    generation_config=config,
    safety_settings=safety
)
print("Model Ä‘Ã£ sáºµn sÃ ng.")

# --- 2. CÃC HÃ€M HELPERS (Giá»¯ nguyÃªn logic cá»§a báº¡n) ---

def get_baseline_prompt(question):
    """Tráº£ vá» prompt Ä‘Æ¡n giáº£n"""
    return f"HÃ£y tráº£ lá»i cÃ¢u há»i sau má»™t cÃ¡ch ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c: {question}"

def get_critique_prompt(question):
    """Tráº£ vá» prompt tá»± pháº£n biá»‡n 3 bÆ°á»›c"""
    return f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI cáº©n trá»ng, luÃ´n kiá»ƒm tra láº¡i thÃ´ng tin.
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i sau báº±ng quy trÃ¬nh 3 bÆ°á»›c.

CÃ¢u há»i: {question}

---
[Báº®T Äáº¦U QUY TRÃŒNH]

**BÆ°á»›c 1: CÃ¢u tráº£ lá»i ban Ä‘áº§u:**
[HÃ£y táº¡o cÃ¢u tráº£ lá»i ban Ä‘áº§u cá»§a báº¡n á»Ÿ Ä‘Ã¢y]

**BÆ°á»›c 2: Tá»± pháº£n biá»‡n:**
[HÃ£y xem xÃ©t láº¡i cÃ¢u tráº£ lá»i á»Ÿ BÆ°á»›c 1. NÃ³ cÃ³ chÃ­nh xÃ¡c khÃ´ng? CÃ³ "hallucinate" Ä‘iá»ƒm nÃ o khÃ´ng? CÃ³ thá»ƒ cáº£i thiá»‡n á»Ÿ Ä‘Ã¢u?]

**BÆ°á»›c 3: CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng (Ä‘Ã£ xÃ¡c minh):**
[Dá»±a trÃªn pháº£n biá»‡n á»Ÿ BÆ°á»›c 2, hÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng, chÃ­nh xÃ¡c nháº¥t.]
"""

def extract_final_answer(critique_text):
    """
    TrÃ­ch xuáº¥t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng tá»« BÆ°á»›c 3 trong self-critique output
    """
    # Regex tÃ¬m "BÆ°á»›c 3" (hoáº·c biáº¿n thá»ƒ) vÃ  láº¥y ná»™i dung sau nÃ³
    patterns = [
        # Match 'BÆ°á»›c 3: [ná»™i dung]'
        r'\*\*BÆ°á»›c 3[:\s]+.*?\*\*[:\s]*(.*?)(?=\n\n|\n\*\*\s*BÆ°á»›c|\Z)', 
        # Match 'CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng: [ná»™i dung]'
        r'CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng[:\s]*\*\*[:\s]*(.*?)(?=\n\n|\n\*\*\s*BÆ°á»›c|\Z)',
        # Match 'BÆ°á»›c 3' khÃ´ng cÃ³ **
        r'BÆ°á»›c 3[:\s]+(.*?)(?=\n\n|\nBÆ°á»›c|\Z)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, critique_text, re.DOTALL | re.IGNORECASE)
        if match:
            answer = match.group(1).strip()
            # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t Ä‘áº§u/cuá»‘i
            answer = answer.strip('*[](){}_- ')
            if answer: # Äáº£m báº£o khÃ´ng pháº£i string rá»—ng
                return answer
    
    # Fallback: Náº¿u khÃ´ng tÃ¬m tháº¥y, cá»‘ gáº¯ng láº¥y dÃ²ng cuá»‘i cÃ¹ng
    last_line = critique_text.split('\n')[-1].strip().strip('*[](){}_- ')
    if last_line:
        return last_line
        
    return critique_text.strip() # Fallback cuá»‘i cÃ¹ng

def calculate_similarity(text1, text2):
    """
    TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 string (0-1)
    """
    return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()

def evaluate_answer(predicted, ground_truth, threshold=0.6):
    """
    ÄÃ¡nh giÃ¡ cÃ¢u tráº£ lá»i dá»± Ä‘oÃ¡n so vá»›i ground truth
    Returns: (is_correct, similarity_score)
    """
    # Xá»­ lÃ½ trÆ°á»ng há»£p predicted hoáº·c ground_truth lÃ  None
    predicted_str = str(predicted).lower().strip()
    ground_truth_str = str(ground_truth).lower().strip()

    if not predicted_str or not ground_truth_str:
        return False, 0.0

    # Exact match (case-insensitive)
    if predicted_str == ground_truth_str:
        return True, 1.0
    
    # Partial match vá»›i similarity
    sim = calculate_similarity(predicted_str, ground_truth_str)
    is_correct = sim >= threshold
    
    return is_correct, sim

# --- 3. HÃ€M CHáº Y THÃ NGHIá»†M CHÃNH ---

def run_and_evaluate_dataset(benchmark_path, similarity_threshold=0.6):
    """
    HÃ m chÃ­nh: Äá»c 1 file benchmark, cháº¡y, Ä‘Ã¡nh giÃ¡, vÃ  in bÃ¡o cÃ¡o.
    """
    
    dataset_name = benchmark_path.stem.replace("benchmark_", "")
    print("\n" + "="*70)
    print(f"ğŸ“Š Báº®T Äáº¦U THÃ NGHIá»†M Vá»šI DATASET: {dataset_name.upper()}")
    print("="*70)

    # Äá»c benchmark CSV
    try:
        benchmark_df = pd.read_csv(benchmark_path)
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {benchmark_path}: {e}")
        return

    results = [] # NÆ¡i lÆ°u trá»¯ táº¥t cáº£ káº¿t quáº£
    
    # Cháº¡y qua tá»«ng hÃ ng trong file benchmark
    for _, item in tqdm(benchmark_df.iterrows(), total=len(benchmark_df), desc=f"   -> Äang cháº¡y {dataset_name}"):
        q = item["question"]
        gt = item["ground_truth"]
        
        # 1. Cháº¡y Baseline
        try:
            prompt_bl = get_baseline_prompt(q)
            response_bl = model.generate_content(prompt_bl)
            answer_bl = response_bl.text.strip()
        except Exception as e:
            answer_bl = f"[Lá»–I: {e}]"

        # 2. Cháº¡y Self-Critique
        try:
            prompt_sc = get_critique_prompt(q)
            response_sc = model.generate_content(prompt_sc)
            answer_sc_full = response_sc.text.strip() # ToÃ n bá»™ output
            answer_sc_final = extract_final_answer(answer_sc_full) # Chá»‰ láº¥y cÃ¢u tráº£ lá»i cuá»‘i
        except Exception as e:
            answer_sc_full = f"[Lá»–I: {e}]"
            answer_sc_final = f"[Lá»–I: {e}]"

        # 3. ÄÃ¡nh giÃ¡ cáº£ 2 phÆ°Æ¡ng phÃ¡p
        bl_correct, bl_sim = evaluate_answer(answer_bl, gt, similarity_threshold)
        sc_correct, sc_sim = evaluate_answer(answer_sc_final, gt, similarity_threshold)

        # 4. LÆ°u káº¿t quáº£
        results.append({
            "question": q,
            "ground_truth": gt,
            "baseline_answer": answer_bl,
            "baseline_correct": bl_correct,
            "baseline_similarity": bl_sim,
            "critique_answer_full": answer_sc_full,
            "critique_answer_final": answer_sc_final,
            "critique_correct": sc_correct,
            "critique_similarity": sc_sim
        })

    # --- 5. PHÃ‚N TÃCH Káº¾T QUáº¢ (CHO DATASET NÃ€Y) ---
    df_results = pd.DataFrame(results)

    # TÃ­nh toÃ¡n cÃ¡c metrics
    baseline_accuracy = df_results['baseline_correct'].sum() / len(df_results) * 100
    critique_accuracy = df_results['critique_correct'].sum() / len(df_results) * 100

    baseline_avg_similarity = df_results['baseline_similarity'].mean()
    critique_avg_similarity = df_results['critique_similarity'].mean()

    # Cáº£i thiá»‡n
    accuracy_improvement = critique_accuracy - baseline_accuracy
    relative_improvement = (accuracy_improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else (100.0 if accuracy_improvement > 0 else 0.0)

    # Äáº¿m sá»‘ cÃ¢u
    sc_better = (df_results['critique_similarity'] > df_results['baseline_similarity']).sum()
    bl_better = (df_results['baseline_similarity'] > df_results['critique_similarity']).sum()
    equal = (df_results['baseline_similarity'] == df_results['critique_similarity']).sum()

    # --- 6. LÆ¯U Káº¾T QUáº¢ RA FILE (CHO DATASET NÃ€Y) ---
    output_csv_file = Path("results") / f"results_{dataset_name}.csv"
    output_txt_file = Path("results") / f"summary_{dataset_name}.txt"
    
    df_results.to_csv(output_csv_file, index=False, encoding="utf-8-sig")

    # --- 7. Táº O VÃ€ IN BÃO CÃO (CHO DATASET NÃ€Y) ---
    summary_report = f"""
=== BÃO CÃO NGHIÃŠN Cá»¨U: REDUCING HALLUCINATIONS ===

** 1. RESEARCH QUESTION **
Liá»‡u ká»¹ thuáº­t Self-Critique prompting cÃ³ giáº£m hallucination vÃ  cáº£i thiá»‡n 
factual accuracy so vá»›i direct prompting khÃ´ng?

** 2. METHODOLOGY **
- Dataset: {dataset_name.upper()} ({len(df_results)} cÃ¢u há»i)
- Model: Gemini 1.5 Pro
- Baseline: Direct prompt Ä‘Æ¡n giáº£n
- Treatment: Self-Critique 3-step prompt (Initial Answer â†’ Critique â†’ Final Answer)
- Evaluation: Similarity score vá»›i ground truth (threshold = {similarity_threshold})

** 3. RESULTS **

Baseline Accuracy:       {baseline_accuracy:.2f}%
Self-Critique Accuracy:  {critique_accuracy:.2f}%
Improvement:             {accuracy_improvement:+.2f}% (absolute)
Relative Improvement:    {relative_improvement:+.2f}%

Average Similarity:
  - Baseline:        {baseline_avg_similarity:.4f}
  - Self-Critique: {critique_avg_similarity:.4f}
  - Difference:      {critique_avg_similarity - baseline_avg_similarity:+.4f}

** 4. CONCLUSION **
"""

    if accuracy_improvement > 0:
        summary_report += f"""
Self-Critique prompting ÄÃƒ THÃ€NH CÃ”NG trong viá»‡c giáº£m hallucination, 
cáº£i thiá»‡n accuracy {accuracy_improvement:.2f}% so vá»›i baseline.
Ká»¹ thuáº­t nÃ y cho tháº¥y tiá»m nÄƒng trong viá»‡c tÄƒng Ä‘á»™ tin cáº­y cá»§a LLM responses.
"""
    elif accuracy_improvement == 0:
        summary_report += f"""
Self-Critique prompting KHÃ”NG CHO THáº¤Y Sá»° KHÃC BIá»†T so vá»›i baseline.
Cáº§n xem xÃ©t ká»¹ hÆ¡n cÃ¡c trÆ°á»ng há»£p cá»¥ thá»ƒ.
"""
    else:
        summary_report += f"""
Self-Critique prompting cho káº¿t quáº£ KÃ‰M HÆ N baseline ({accuracy_improvement:.2f}%) trong thÃ­ nghiá»‡m nÃ y.
CÃ³ thá»ƒ prompt design chÆ°a tá»‘i Æ°u hoáº·c model gáº·p khÃ³ khÄƒn trong viá»‡c tá»± sá»­a lá»—i.
"""

    summary_report += f"""

** 5. DETAILED BREAKDOWN **
Self-Critique performs better: {sc_better} cases ({sc_better/len(df_results)*100:.1f}%)
Baseline performs better:      {bl_better} cases ({bl_better/len(df_results)*100:.1f}%)
Equal performance:           {equal} cases ({equal/len(df_results)*100:.1f}%)

===================================================
"""

    # LÆ°u summary report
    with open(output_txt_file, "w", encoding="utf-8") as f:
        f.write(summary_report)

    # In bÃ¡o cÃ¡o ra console
    print(summary_report)
    print(f"ğŸ’¾ Chi tiáº¿t Ä‘áº§y Ä‘á»§ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_csv_file}")
    print(f"ğŸ“„ BÃ¡o cÃ¡o tÃ³m táº¯t Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_txt_file}")


# --- 4. HÃ€M MAIN Äá»‚ CHáº Y Táº¤T Cáº¢ DATASET ---
if __name__ == "__main__":
    
    # Táº¡o thÆ° má»¥c data/ vÃ  results/ náº¿u chÆ°a cÃ³
    Path("data").mkdir(exist_ok=True)
    Path("results").mkdir(exist_ok=True)

    # TÃ¬m táº¥t cáº£ cÃ¡c file benchmark Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹
    benchmark_files = list(Path("data").glob("benchmark_*.csv"))
    
    if not benchmark_files:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file benchmark nÃ o trong thÆ° má»¥c 'data/'.")
        print("â¡ï¸  Vui lÃ²ng cháº¡y 'python app/prepare_data.py' trÆ°á»›c tiÃªn.")
        exit()
        
    print(f"TÃ¬m tháº¥y {len(benchmark_files)} bá»™ dataset benchmark Ä‘á»ƒ cháº¡y:")
    for f in benchmark_files:
        print(f"  - {f.name}")

    # Láº·p qua tá»«ng file benchmark vÃ  cháº¡y thÃ­ nghiá»‡m
    for benchmark_path in benchmark_files:
        run_and_evaluate_dataset(benchmark_path, similarity_threshold=0.6)

    print("\nğŸ‰ğŸ‰ğŸ‰ Táº¥t cáº£ 5 thÃ­ nghiá»‡m Ä‘Ã£ hoÃ n táº¥t! ğŸ‰ğŸ‰ğŸ‰")
    print("Kiá»ƒm tra thÆ° má»¥c 'results/' Ä‘á»ƒ xem 5 file CSV vÃ  5 file TXT bÃ¡o cÃ¡o.")
